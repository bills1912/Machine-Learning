{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql import functions as pfs\r\n",
    "\r\n",
    "spark = SparkSession.builder.appName(\"CreditScoringAnalysis\").getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = spark.read.csv('credit_scoring.csv',\r\n",
    "                    sep=\",\",\r\n",
    "                    header=True,\r\n",
    "                    quote='\"',\r\n",
    "                    inferSchema=True,)\r\n",
    "df.show(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df.drop('kode_kontrak')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\r\n",
    "\r\n",
    "kpr_encoder = StringIndexer(inputCol='kpr_aktif', outputCol='kpr_encode').fit(df)\r\n",
    "df = kpr_encoder.transform(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rro_encoder = StringIndexer(inputCol='rata_rata_overdue', outputCol='rro').fit(df)\r\n",
    "df = rro_encoder.transform(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df.drop('kpr_aktif','rata_rata_overdue')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "requires = ['pendapatan_setahun_juta', 'durasi_pinjaman_bulan', 'jumlah_tanggungan', 'risk_rating', 'kpr_encode', 'rro']\r\n",
    "\r\n",
    "vec_asm = VectorAssembler(inputCols=requires, outputCol='features')\r\n",
    "df = vec_asm.transform(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training, testing = df.randomSplit([0.75, 0.25])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\r\n",
    "\r\n",
    "dtc = DecisionTreeClassifier(featuresCol='features', labelCol='risk_rating')\r\n",
    "dtc = dtc.fit(training)\r\n",
    "\r\n",
    "y_predict = dtc.transform(testing)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = y_predict.na.drop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\r\n",
    "\r\n",
    "multi_eval = MulticlassClassificationEvaluator(labelCol='risk_rating', metricName='accuracy')\r\n",
    "print(\"DTC model score using pyspark:\", multi_eval.evaluate(y_pred))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "f23faf4bfe871c203c8bec80520af5927fc7cb1ae3bd834ddf554ee587ad1c05"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}